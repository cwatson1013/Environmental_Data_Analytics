---
title: "11: Generalized Linear Models"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## LESSON OBJECTIVES
1. Describe the components of the generalized linear model (GLM)
2. Apply special cases of the GLM to real datasets
3. Interpret and report the results of GLMs in publication-style formats

## SET UP YOUR DATA ANALYSIS SESSION

```{r, message = FALSE, warning = FALSE}
getwd()
library(tidyverse)

PeterPaul.nutrients <- read.csv("./Data/Processed/NTL-LTER_Lake_Nutrients_PeterPaul_Processed.csv")
EPAair <- read.csv("./Data/Processed/EPAair_O3PM25_3sites1718_processed.csv")

# Set date to date format
EPAair$Date <- as.Date(EPAair$Date, format = "%Y-%m-%d")
PeterPaul.nutrients$sampledate <- as.Date(PeterPaul.nutrients$sampledate, format = "%Y-%m-%d")

# remove negative values for depth_id
PeterPaul.nutrients <- filter(PeterPaul.nutrients, depth_id > 0)

# set depth_id to factor
PeterPaul.nutrients$depth_id <- as.factor(PeterPaul.nutrients$depth_id)

mytheme <- theme_classic(base_size = 14) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)

```

## GENERALIZED LINEAR MODELS 

The one-sample test (model of the mean), two-sample t-test, analysis of variance (ANOVA), and linear regression are all special cases of the **generalized linear model** (GLM). The GLM also includes analyses not covered in this class, including logistic regression, multinomial regression, chi square, and log-linear models. The common characteristic of general linear models is the expression of a continuous response variable as a linear combination of the effects of categorical or continuous explanatory variables, plus an error term that expresses the random error associated with the coefficients of all explanatory variables. The explanatory variables comprise the deterministic component of the model, and the error term comprises the stochastic component of the model. Historically, artificial distinctions were made between linear models that contained categorical and continuous explanatory variables, but this distinction is no longer made. The inclusion of these models within the umbrella of the GLM allows models to fit the main effects of both categorical and continuous explanatory variables as well as their interactions. 

### Choosing a model from your data: A "cheat sheet"

**T-test:** Continuous response, one categorical explanatory variable with two categories (or comparison to a single value if a one-sample test).

**One-way ANOVA (Analysis of Variance):** Continuous response, one categorical explanatory variable with more than two categories.

**Two-way ANOVA (Analysis of Variance)** Continuous response, two categorical explanatory variables.

**Single Linear Regression** Continuous response, one continuous explanatory variable.

**Multiple Linear Regression** Continuous response, two or more continuous explanatory variables.

**ANCOVA (Analysis of Covariance)** Continuous response, categorical explanatory variable(s) and  continuous explanatory variable(s).

If multiple explanatory variables are chosen, they may be analyzed with respect to their **main effects** on the model (i.e., their separate impacts on the variance explained) or with respsect to their **interaction effects,** the effect of interacting explanatory variables on the model. 

### Assumptions of the GLM

The GLM is based on the assumption that the data approximate a normal distribution (or a linearly transformed normal distribution). We will discuss the non-parametric analogues to several of these tests if the assumptions of normality are violated. For tests that analyze categorical explanatory variables, the assumption is that the variance in the response variable is equal among groups. Note: environmental data often violate the assumptions of normality and equal variance, and we will often proceed with a GLM even if these assumptions are violated. In this situation, you must justify your decision. 

## T-TEST AND ONE-WAY ANOVA
### One-sample t-test
The object of a one sample test is to test the null hypothesis that the mean of the group is equal to a specific value. For example, we might ask ourselves (from the EPA air quality processed dataset): 

Are Ozone levels below the threshold for "good" AQI index (0-50)?

```{r}
summary(EPAair$Ozone)

# Evaluate assumption of normal distribution
shapiro.test(EPAair$Ozone) #evaluates whether data is well approximated by a normal distribution; p-value less than 0.05, data is not normally distributed
ggplot(EPAair, aes(x = Ozone)) +
  geom_histogram(stat = "count") #it has a right tail which is common in environmental data
qqnorm(EPAair$Ozone); qqline(EPAair$Ozone)

O3.onesample <- t.test(EPAair$Ozone, mu = 50, alternative = "less") #tell R what we are comparing data too - are they less than 50?; alternative ="less" will tell R that you are looking at AQI less than 50
O3.onesample
```

What information does the output give us? How might we report this information in a report?

> ANWSER: p-value is less than 0.05, which means that we can reject our null hypothesis; t-score (t=-41) --> further away from 0, the lower the probability is going to be; mean of x = 36.9 which means we have an equation of y = 36.9 + error 
> Ozone AQI values in 2017 - 2018 were significantly lower than 50 (one sample t-test; t = -41.9, df = 1084, p < 0.0001).
> (t=-41.9, df = 1084); we are 95% confident that the true mean lies between 

### Two-sample t-test
The two-sample *t* test is used to test the hypothesis that the mean of two samples is equivalent. Unlike the one-sample tests, a two-sample test requires a second assumption that the variance of the two groups is equivalent. Are Ozone levels different between Blackstone and Bryson City?

```{r}
shapiro.test(EPAair$Ozone[EPAair$Site.Name == "Blackstone"])
shapiro.test(EPAair$Ozone[EPAair$Site.Name == "Bryson City"])
var.test(EPAair$Ozone ~ EPAair$Site.Name) #variance test to compare variances and see if they are similar or not; var.test gives us an F statistic and a p-value that indicates whether the prob the variances are the same; variances between two populations is different; 

ggplot(EPAair, aes(x = Ozone, color = Site.Name)) +
  geom_freqpoly(stat = "count")

# Format as a t-test
O3.twosample <- t.test(EPAair$Ozone ~ EPAair$Site.Name)
O3.twosample
O3.twosample$p.value

# Format as a GLM
O3.twosample2 <- lm(EPAair$Ozone ~ EPAair$Site.Name)
summary(O3.twosample2)
```
### Non-parametric equivalent of t-test: Wilcoxon test

When we wish to avoid the assumption of normality, we can apply *distribution-free*, or non-parametric, methods in the form of the Wilcoxon rank sum (Mann-Whitney) test. The Wilcoxon test replaces the data by their rank and calculates the sum of the ranks for each group. Notice that the output of the Wilcoxon test is more limited than its parametric equivalent.

```{r}
O3.onesample.wilcox <- wilcox.test(EPAair$Ozone, mu = 50, alternative = "less")
O3.onesample.wilcox #get a V value when you run this on a one-sided t-test
O3.twosample.wilcox <- wilcox.test(EPAair$Ozone ~ EPAair$Site.Name)
O3.twosample.wilcox #get a W value when you run this on a two-sided t-test; just tells us that the means are different, but nothing more
```

### One-way ANOVA
A one-way ANOVA is the same test in practice as a two-sample t-test but for three or more groups. In R, we can  run the model with the function `lm` or `aov`, the latter of which which will allow us to run post-hoc tests to determine pairwise differences.

Are PM2.5 levels different between Blackstone, Bryson City, and Triple Oak?
```{r}
shapiro.test(EPAair$PM2.5[EPAair$Site.Name == "Blackstone"]) #only want to look at PM2.5 where site name is Blackstone
shapiro.test(EPAair$PM2.5[EPAair$Site.Name == "Bryson City"])
shapiro.test(EPAair$PM2.5[EPAair$Site.Name == "Triple Oak"])

ggplot(EPAair, aes(x = PM2.5, color = Site.Name)) +
  geom_freqpoly(stat = "count")
qqnorm(EPAair$PM2.5); qqline(EPAair$PM2.5) #departure from normality happens at the tails, which we see in this plot

bartlett.test(EPAair$PM2.5 ~ EPAair$Site.Name) #this test tests for equal variance of more than two populations; K-squared value; p-value here is 0.08 which tells us that the variances are not significantly different from each other

# Format as a GLM
PM2.5.anova <- lm(EPAair$PM2.5 ~ EPAair$Site.Name) #response variable by our explanatory variable
summary(PM2.5.anova)

# Format as an aov
PM2.5.anova2 <- aov(EPAair$PM2.5 ~ EPAair$Site.Name) #will tell us if site is a significant predictor, but won't tell us which sites
summary(PM2.5.anova2)

# Run a post-hoc test for pairwise differences
TukeyHSD(PM2.5.anova2) #this will tell you which sites are significantly different; Triple Oak and Bryson City are not significantly different because p-value is greater than 0.05 in the TukeyHSD test
plot(TukeyHSD(PM2.5.anova2))

# Plot the results
# How might you edit this graph to make it attractive?
# How might you illustrate significant differences?
PM2.5.anova.plot <- ggplot(EPAair, aes(x = Site.Name, y = PM2.5)) +
  geom_violin(draw_quantiles = 0.5)
print(PM2.5.anova.plot) #can plot a boxplot, dot plot, violin plot

#Blackstone has a higher mean than Bryson and Triple Oak; Bryson and Triple Oak don't have much difference between the means; distributions are different though
```
What information does the output give us? How might we report this information in a report?

> ANSWER: Show that Blackstone is more significant by putting a star by the plot on the graph, or put Blackstone in group A, and Bryson and Triple Oak in group B. Can use geom_text to include text on the graph. Blackstone has higher concentrations of PM2.5 on average than Bryson City and Triple Oak (ANOVA; F = 16.16, df = 1898, p < 0.0001). Bryson and Triple Oak did not have significantly different PM 2.5 values over the study period. 

### Non-parametric equivalent of ANOVA: Kruskal-Wallis Test
As with the Wilcoxon test, the Kruskal-Wallis test is the non-parametric counterpart to the one-way ANOVA. Here, the data from two or more independent samples are replaced with their ranks without regard to the grouping AND based on the between-group sum of squares calculations. 

For multiple comparisons, a p-value < 0.05 indicates that there is a significant difference between groups, but it does not indicate which groups, or in this case, months, differ from each other.

To analyze specific pairs in the data, you must use a *post hoc* test. These include the Dunn's test, a pairwise Mann-Whitney with the Bonferroni correction, or the Conover-Iman test.

```{r}
PM2.5.kw <- kruskal.test(EPAair$PM2.5 ~ EPAair$Site.Name)
PM2.5.kw #chi-squared value, df, and p-value will come out when you run a kruskal.test

# There are two functions to run the Dunn Test
# dunn.test(EPAair$PM2.5, EPAair$Site.Name, kw = T, 
#           table = F, list = T, method = "holm", altp = T)   #From package dunn.test
# dunnTest(EPAair$PM2.5, EPAair$Site.Name)                    #From package FSA
```

## TWO-WAY ANOVA
### Main effects
A two-way ANOVA allows us to examine the effects of two categorical explanatory variables on a continuous response variable. Let's look at the NTL-LTER nutrient dataset for Peter and Paul lakes. What if we wanted to know if total nitrogen concentrations differed based on lake and depth? 

```{r}
TNanova.main <- lm(PeterPaul.nutrients$tn_ug ~ PeterPaul.nutrients$lakename + PeterPaul.nutrients$depth_id) #important to divide depth_id into factor earlier
summary(TNanova.main) #ID 5 & 6 are not significantly different from intercept

TNanova.main2 <- aov(PeterPaul.nutrients$tn_ug ~ PeterPaul.nutrients$lakename + PeterPaul.nutrients$depth_id)
summary(TNanova.main2) #shows that lakename and depth ID are significant in telling us they are both significant

TukeyHSD(TNanova.main2)

# Plot the results
# How might you edit this graph to make it attractive?
# How might you illustrate significant differences?
TNanova.plot <- ggplot(PeterPaul.nutrients, aes(x = lakename, y = tn_ug, color = depth_id)) +
  geom_boxplot()
print(TNanova.plot) #means for Peter Lake are a little higher than those from Paul lake; lots of nitrogen at deeper depths
```

### Interaction effects
We may expect the effects of lake and depth to be dependent on each other. For instance, since depth_id is standardized across lakes, the concentrations at each depth_id might depend on which lake is sampled. In this case, we might choose to run an interaction effects two-way ANOVA, which will examine the individual effects of the explanatory variables as well as the interaction of the explanatory variables.

The output gives test statistics for each explanatory variable as well as the interaction effect of the explanatory variables. If the p-value for the interaction effect is less than 0.05, then we would consider the interaction among the explanatory variables to be significant.

```{r}
TNanova.interaction <- aov(PeterPaul.nutrients$tn_ug ~ PeterPaul.nutrients$lakename * PeterPaul.nutrients$depth_id) #interaction effect indicated by the *
summary(TNanova.interaction) #do we have a correlation between these two levels? We do from this test; significant interaction we are only going to interpret the effects of those interactions

```

If the interaction is significant, we interpret pairwise differences for the interaction. If the interaction is not significant, we interpret differences for the main effects only.
```{r}
TukeyHSD(TNanova.interaction)
```

Pairs are considered to be in the same grouping if the p-value for that pairing is > 0.05. It is easy to see that this grouping process can become complicated when many factors are present for each variable! For a challenge, try writing code that will generate groupings for each factor level in the dataset using the `glht` function in the `multcomp` package. 

### Exercise

Run the same tests and visualizations (main and interaction effects two-way ANOVA) for total phosphorus concentrations. How do your results compare for the different nutrients?

```{r}

```


